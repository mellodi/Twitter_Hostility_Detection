{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bahar Radfar - last update: May-2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook presents how to train hostility detection model using both LSTM and Logistic Regression classification model. The main purpose is to understand the influence of relationship between the conversation partisipents on both the presence and intensity of hostile comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "import cv2\n",
    "import json\n",
    "import math\n",
    "import nltk\n",
    "import keras\n",
    "import string\n",
    "import logging\n",
    "import pickle as p\n",
    "import regex as re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from operator import itemgetter\n",
    "from collections import Counter\n",
    "from IPython import get_ipython\n",
    "from subprocess import check_output\n",
    "from string import punctuation, ascii_lowercase\n",
    "\n",
    "\n",
    "\n",
    "from numpy import random\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "\n",
    "from emoji_function import demojize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.datasets import imdb\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import CuDNNLSTM, Bidirectional, LSTM\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Input, Dense, Activation, Dropout,SpatialDropout1D\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.manifold import TSNE\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import sklearn.preprocessing as preprocessing\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "import sklearn.feature_extraction as feature_extraction\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "\n",
    "\n",
    "## Plotly\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "py.init_notebook_mode(connected=True)\n",
    "\n",
    "\n",
    "from myUtility import dataPrepration, cleanText, replaceThreeOrMore, preprocess, auroc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Deep learning model with LSTM layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HostilityDetectionModel:\n",
    "    \"\"\"\n",
    "        The hostility ditection LSTM model \n",
    "        Params:\n",
    "          args....A dictionary, representing configuration values necessery for the model\n",
    "        Returns:\n",
    "            The trained and tested model\n",
    "          \n",
    "       \n",
    "    \"\"\"\n",
    "    def __init__(self, args):\n",
    "        self.lstm_cells = args['lstm_cells'] #265\n",
    "        self.opt = args['opt'] #keras.optimizers.Adam(lr=1e-3, decay=0.1)\n",
    "        self.batch_size = args['batch_size']\n",
    "        self.epochs = args['epochs']\n",
    "        self.loss = args['loss'] #categorical_crossentropy\n",
    "        self.comment_size = args['comment_size']  #(65)\n",
    "        self.feature_size = args['feature_size'] #dim\n",
    "        self.vocab_size = args['vocab_size']\n",
    "        self.embedding_matrix = args['embedding_matrix']\n",
    "        self.save_path = args['save_path']\n",
    "        self.tensorBoard_directory = args[\"tensorBoard_directory\"]\n",
    "        \n",
    "        self.build_model()\n",
    "    \n",
    "    def build_model(self):\n",
    "        \n",
    "        comment_input = Input(shape=(self.comment_size,), name=\"tweet_content\")\n",
    "        features_input = Input(shape=(self.feature_size,), name=\"features\")\n",
    "        \n",
    "        wv_layer = Embedding(self.vocab_size, 200, weights=[self.embedding_matrix], \n",
    "                             input_length=self.comment_size, trainable=False)\n",
    "        embedded_sequences = wv_layer(comment_input)\n",
    "\n",
    "        embedded_sequences = SpatialDropout1D(0.2)(embedded_sequences)\n",
    "        x = keras.layers.Bidirectional(keras.layers.CuDNNLSTM(self.lstm_cells, \n",
    "                                                              return_sequences=True))(embedded_sequences)\n",
    "        x = keras.layers.Bidirectional(keras.layers.CuDNNLSTM(self.lstm_cells, \n",
    "                                                              return_sequences=False, return_state=False))(x)\n",
    "        y = keras.layers.concatenate([x, features_input])\n",
    "        y = keras.layers.Dense(256, activation=\"relu\")(y)\n",
    "        y = keras.layers.Dropout(0.20)(y)\n",
    "        x = keras.layers.Dense(2, activation=\"softmax\")(y)\n",
    "        self.model = keras.models.Model(inputs=[comment_input, features_input], outputs=x)\n",
    "        self.model.compile(loss= self.loss, optimizer=self.opt, metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "    \n",
    "    def train(self, data_train, feature_train, y_train, train_idx, val_idx):\n",
    "        \"\"\"\n",
    "            The training process of the model\n",
    "            Params:\n",
    "              data_train......A list of lists, each representing the tokenized version of a tweet\n",
    "              feature_train...A list of lists, each representing the tokenized version of a string\n",
    "                              representing the features\n",
    "              y_train.........A list of lists, each representing the one_hot version of the label\n",
    "              train_idx.......A list of intigers, representing the index of items from data_train \n",
    "                              which should be considered as the training set\n",
    "              val_idx.........A list of intigers, representing the index of items from data_train \n",
    "                              which should be considered as the validation set\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        data_train_cv = data_train[train_idx]\n",
    "        feature_train_cv = feature_train[train_idx]\n",
    "        y_train_cv = y_train[train_idx]\n",
    "\n",
    "        data_valid_cv = data_train[val_idx]\n",
    "        feature_valid_cv = feature_train[val_idx]\n",
    "        y_valid_cv= y_train[val_idx]\n",
    "        \n",
    "        y_train_cv = keras.utils.to_categorical([0 if j == 0 else 1 for j in y_train_cv], num_classes=2)\n",
    "        y_valid_cv = keras.utils.to_categorical([0 if j == 0 else 1 for j in y_valid_cv], num_classes=2)\n",
    "\n",
    "        \n",
    "        y_integers = np.argmax(y_train_cv, axis=1)\n",
    "        class_weights = class_weight.compute_class_weight('balanced', np.unique(y_integers), y_integers)\n",
    "        d_class_weights = dict(enumerate(class_weights))\n",
    "        sample_weights = class_weight.compute_sample_weight('balanced', y_integers)\n",
    "        \n",
    "        \n",
    "        tensor_board = TensorBoard(log_dir=self.tensorBoard_directory, histogram_freq=0, write_graph=True, \n",
    "                                   write_images=True)\n",
    "        callbacks = [tensor_board]\n",
    "        hist = self.model.fit(\n",
    "                            [data_train_cv, feature_train_cv],\n",
    "                             y_train_cv, \n",
    "                             epochs=self.epochs, \n",
    "                             batch_size= self.batch_size, \n",
    "                             validation_data = ([data_valid_cv, feature_valid_cv], y_valid_cv),\n",
    "                             shuffle=True, \n",
    "                             sample_weight=sample_weights,\n",
    "                             callbacks=callbacks)\n",
    "        self.model.save(self.save_path)\n",
    "        \n",
    "    def test(self, data_test, feature_test):\n",
    "        \"\"\"\n",
    "            The testing process of the model\n",
    "            Params:\n",
    "              data_test......A list of lists, each representing the tokenized version of a tweet\n",
    "              feature_test...A list of lists, each representing the tokenized version of a string\n",
    "                              representing the features\n",
    "             Returns:\n",
    "                 the list of lists, each representing predicted probability of a tweet\n",
    "\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.model = keras.models.load_model(self.save_path)\n",
    "            print(\"Loaded trained model!\")\n",
    "        except:\n",
    "            print(\"No previous model found, using untrained model.\")\n",
    "        print(\"Predicting...\")\n",
    "        self.y_pred = model.predict([test_data, test_groups], batch_size=self.batch_size)\n",
    "        print(\"Prediction done!\")\n",
    "        return self.y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepLearningModel(rootAddress, fileName, max_length=65, num_folds=5):\n",
    "    \"\"\"\n",
    "        Featured model using both the length and relationship category of each tweet as a classifier feature.\n",
    "        Reading the data and running the model using k_fold cross_validation \n",
    "        Params:\n",
    "            rootAddress...A string representing the rood directory.\n",
    "            fileName......A list of file names, representing the different relationship categories.\n",
    "            max_length....An intiger, representing the max number of words to pick from each tweet \n",
    "            num_folds.....An intiger, representing the number of k_fold\n",
    "        \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # read the data\n",
    "    filePath = rootAddress + fileName + '.csv'\n",
    "    data= pd.read_csv(filePath,dtype={'group': str}).drop_duplicates('id', keep='first')\n",
    "    \n",
    "    tweets = data['text'].map(lambda x: preprocess(x))\n",
    "    labels = data['hostile'].values\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(tweets)\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    encoded_docs = tokenizer.texts_to_sequences(tweets)\n",
    "    text = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "    \n",
    "    # convert the relationship values to it's int representation\n",
    "    relationshipDic = {\n",
    "        '0': 0 ,\n",
    "        '1': 1,\n",
    "        '10': 2,\n",
    "        '11': 3\n",
    "    }\n",
    "    results = []\n",
    "    relationshipType = data['group'].values\n",
    "    \n",
    "    for i in range(len(text)):\n",
    "        res ={\n",
    "            'text':text[i],\n",
    "            'group':relationshipType[i],\n",
    "            'feature': np.array([relationshipDic[relationshipType[i]], len(text[i])]),\n",
    "            'y':labels[i]\n",
    "        }\n",
    "        results.append(res)\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # represent the different relation and len as a unique value in a string format \n",
    "    featuresString = ['relation_%s len_%s' %\n",
    "                (x[0], x[1]) for x in results_df['feature'].values]\n",
    "    \n",
    "    vect = feature_extraction.text.CountVectorizer(binary = True, min_df = 0.0005)\n",
    "    features = vect.fit_transform(featuresString).toarray()\n",
    "\n",
    "    results_df['feature'] = [features[i] for i in range(len(text))]\n",
    "    train_data  = results_df.as_matrix()\n",
    "    \n",
    "    # Use the glove w2v as the embeding \n",
    "    embeddings_index = dict()\n",
    "    f = open('glove.6B.300d.word2vec.txt')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "    \n",
    "    # create a weight matrix for words in training docs\n",
    "    embedding_matrix = zeros((vocab_size, 200))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    # dividing the data into 15% test and 85% (train and validation)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(train_data, labels, test_size=0.15, random_state = 123)\n",
    "    \n",
    "    # creating a list of touples for each fold, each representing the train indices and validation indicies \n",
    "    # for that round\n",
    "    folds = list(StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=1).split(X_train, y_train))\n",
    "\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    feature_train = np.array([i for i in X_train[0].values])\n",
    "    data_train = np.array(X_train[2].values.tolist())  \n",
    "    \n",
    "    # set the training config arguments\n",
    "    args = {\n",
    "        \"lstm_cells\": 265,\n",
    "        \"opt\":keras.optimizers.Adam(),\n",
    "        \"batch_size\": 64,\n",
    "        \"epochs\":40,\n",
    "        \"loss\": \"categorical_crossentropy\",\n",
    "        \"comment_size\": max_length,\n",
    "        \"feature_size\": feature_train[0].shape[0],\n",
    "        \"vocab_size\":vocab_size,\n",
    "        \"embedding_matrix\":embedding_matrix,\n",
    "        \"save_path\": \"FeaturedModel.model\",\n",
    "        \"tensorBoard_directory\": rootAddress\n",
    "    }\n",
    "    \n",
    "    \n",
    "    X_test =  pd.DataFrame(X_test)\n",
    "    \n",
    "    # impliment the cross_validation manually\n",
    "    for j, (train_idx, val_idx) in enumerate(folds):\n",
    "\n",
    "        print('\\nFold ',j)\n",
    "        \n",
    "        model = HostilityDetectionModel(args)\n",
    "        model.train(data_train, feature_train, y_train, train_idx, val_idx)\n",
    "        \n",
    "        data_test = np.array(X_test[2].values.tolist())\n",
    "        feature_test = np.array([i for i in X_test[0].values])\n",
    "        y_pred = model.test(data_test, feature_test)\n",
    "        y_pred = [1 if p[1] > 0.555 else 0 for p in y_pred]\n",
    "        print(f1_score(y_test, y_pred, average='weighted'))\n",
    "        print(metrics.classification_report(y_test, y_pred, digits=4))\n",
    "        fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, y_pred)\n",
    "        auc_rf = auc(fpr_rf, tpr_rf)\n",
    "        print(auc_rf)\n",
    "        print(\"-------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseModel(rootAddress, fileName, max_length=65, num_folds=5):\n",
    "    \"\"\"\n",
    "        Base model using the only the length of each tweet as a classifier feature.\n",
    "        Reading the data and running the model using k_fold cross_validation \n",
    "        Params:\n",
    "            rootAddress...A string representing the rood directory.\n",
    "            fileName......A list of file names, representing the different relationship categories.\n",
    "            max_length....An intiger, representing the max number of words to pick from each tweet \n",
    "            num_folds.....An intiger, representing the number of k_fold\n",
    "        \n",
    "\n",
    "    \"\"\"\n",
    "    # read the data\n",
    "    filePath = rootAddress + fileName + '.csv'\n",
    "    data= pd.read_csv(filePath,dtype={'group': str}).drop_duplicates('id', keep='first')\n",
    "    \n",
    "\n",
    "    tweets = data['text'].map(lambda x: preprocess(x))\n",
    "    labels = data['hostile'].values\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(tweets)\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    encoded_docs = tokenizer.texts_to_sequences(tweets)\n",
    "    text = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "    \n",
    "    results = []\n",
    "    relationshipType = data['group'].values\n",
    "    \n",
    "    for i in range(len(text)):\n",
    "        res ={\n",
    "            'text':text[i],\n",
    "            'group':relationshipType[i],\n",
    "            'feature': np.array([len(text[i])]),\n",
    "            'y':labels[i]\n",
    "        }\n",
    "        results.append(res)\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # represent the different length as a unique value in a string format \n",
    "    featuresString = ['len_%s' %\n",
    "                (x[0]) for x in results_df['feature'].values]\n",
    "    \n",
    "    vect = feature_extraction.text.CountVectorizer(binary = True, min_df = 0.0005)\n",
    "    features = vect.fit_transform(featuresString).toarray()\n",
    "\n",
    "    results_df['feature'] = [features[i] for i in range(len(text))]\n",
    "    train_data  = results_df.as_matrix()\n",
    "    \n",
    "    # Use the glove w2v as the embeding\n",
    "    embeddings_index = dict()\n",
    "    f = open('glove.6B.300d.word2vec.txt')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "    \n",
    "    # create a weight matrix for words in training docs\n",
    "    embedding_matrix = zeros((vocab_size, 200))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    # dividing the data into 15% test and 85% (train and validation)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(train_data, labels, test_size=0.15, random_state = 123)\n",
    "    \n",
    "    # creating a list of touples for each fold, each representing the train indices and validation indicies \n",
    "    # for that round\n",
    "    folds = list(StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=1).split(X_train, y_train))\n",
    "\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    feature_train = np.array([i for i in X_train[0].values])\n",
    "    data_train = np.array(X_train[2].values.tolist())  \n",
    "    \n",
    "    # set the training config arguments\n",
    "    args = {\n",
    "        \"lstm_cells\": 265,\n",
    "        \"opt\":keras.optimizers.Adam(),\n",
    "        \"batch_size\": 64,\n",
    "        \"epochs\":40,\n",
    "        \"loss\": \"categorical_crossentropy\",\n",
    "        \"comment_size\": max_length,\n",
    "        \"feature_size\": feature_train[0].shape[0],\n",
    "        \"vocab_size\":vocab_size,\n",
    "        \"embedding_matrix\":embedding_matrix,\n",
    "        \"save_path\": \"BaseModel.model\",\n",
    "        \"tensorBoard_directory\": rootAddress\n",
    "    }\n",
    "    \n",
    "    \n",
    "    \n",
    "    X_test =  pd.DataFrame(X_test)\n",
    "    \n",
    "    # impliment the cross_validation manually\n",
    "    for j, (train_idx, val_idx) in enumerate(folds):\n",
    "\n",
    "        print('\\nFold ',j)\n",
    "        \n",
    "        model = HostilityDetectionModel(args)\n",
    "        model.train(data_train, feature_train, y_train, train_idx, val_idx)\n",
    "        \n",
    "        data_test = np.array(X_test[2].values.tolist())\n",
    "        feature_test = np.array([i for i in X_test[0].values])\n",
    "        y_pred = model.test(data_test, feature_test)\n",
    "        y_pred = [1 if p[1] > 0.555 else 0 for p in y_pred]\n",
    "        print(f1_score(y_test, y_pred, average='weighted'))\n",
    "        print(metrics.classification_report(y_test, y_pred, digits=4))\n",
    "        fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, y_pred)\n",
    "        auc_rf = auc(fpr_rf, tpr_rf)\n",
    "        print(auc_rf)\n",
    "        print(\"-------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_informative_feature_for_class(vectorizer, classifier, classlabel, n=10):\n",
    "    \"\"\"\n",
    "        Finding the most correlated feature regarding each label\n",
    "        Params:\n",
    "            vectorizer...An obj, representing the vecterized used in the model\n",
    "            classifier...An obj, representing the classifier used in the model\n",
    "            classlabel...An list of intigers, representing the classifier labels\n",
    "            n............An intiger, representing the number of correlated feature\n",
    "                        to show for each label\n",
    "        \n",
    "\n",
    "    \"\"\"\n",
    "    labelid = list(classifier.classes_).index(classlabel)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    bottomn = sorted(zip(classifier.coef_[labelid], feature_names))[-n:]\n",
    "    topn = sorted(zip(classifier.coef_[labelid], feature_names))[:n]\n",
    "    for coef, feat in topn:\n",
    "        print(classlabel, feat,  coef*-1) \n",
    "    print(\"---------\")\n",
    "    for coef, feat in bottomn:\n",
    "        print(1 - classlabel, feat, coef) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.read_csv('sampleData7.csv',dtype={'group': str}).drop_duplicates('id', keep='first')\n",
    "X = data['text'].map(lambda x: preprocess(x))\n",
    "y = data['hostile'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) LR model using only the tweet text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7266470009832842\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.74      0.73       502\n",
      "           1       0.74      0.71      0.72       515\n",
      "\n",
      "   micro avg       0.73      0.73      0.73      1017\n",
      "   macro avg       0.73      0.73      0.73      1017\n",
      "weighted avg       0.73      0.73      0.73      1017\n",
      "\n"
     ]
    }
   ],
   "source": [
    "col_names = ['tweet', 'hostility']\n",
    "y = y.astype('int')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state = 123)\n",
    "nb = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('LR', LogisticRegressionCV(cv=5, max_iter=4000,random_state=0, solver='lbfgs',multi_class='multinomial', class_weight='balanced')),\n",
    "              ])\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The top 10 corelated word for hostile/non_hostile groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 emoji_face_with_tears_of_joy 0.9361011241773141\n",
      "0 lmao 0.8541578648934544\n",
      "0 love 0.8040690711430389\n",
      "0 holy 0.7629214371135057\n",
      "0 shit 0.7544601087310946\n",
      "0 emoji_skull 0.6714727079002918\n",
      "0 emoji_heavy_black_heart 0.6592753496746697\n",
      "0 emoji_hundred_points_symbol 0.6165805429744967\n",
      "0 lol 0.5943807699790422\n",
      "0 it 0.5785230808921368\n",
      "---------\n",
      "1 dumb 0.7873820465990385\n",
      "1 faggot 0.8039466783083321\n",
      "1 dick 0.888475887009469\n",
      "1 fucking 0.9126462590262244\n",
      "1 pussy 0.9421538158570509\n",
      "1 shut 1.0960886597839976\n",
      "1 ass 1.4237217855046846\n",
      "1 retard 1.6826023013771572\n",
      "1 twat 2.096198498505101\n",
      "1 cunt 2.6973273854289115\n"
     ]
    }
   ],
   "source": [
    "classifier = nb.named_steps['LR']\n",
    "vectorizer = nb.named_steps['vect']\n",
    "most_informative_feature_for_class(vectorizer, classifier, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) LR model using text + relationship category + length of each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:30: FutureWarning:\n",
      "\n",
      "Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = dataPrepration(data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_data, y, test_size=0.15, random_state = 123)\n",
    "X_train = pd.DataFrame(X_train)\n",
    "X_train.columns = ['features', 'group', 'text', 'label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize and vertoriez the text and prepare it for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:10: FutureWarning:\n",
      "\n",
      "Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train['text'].values)\n",
    "count_vect.vocabulary_.get(u'algorithm')\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "train_data = []\n",
    "bagOfWords = X_train_tfidf.toarray()\n",
    "features = X_train['features'].as_matrix().tolist()\n",
    "for i in range(len(bagOfWords)):\n",
    "    res ={\n",
    "        'bagOfWords': bagOfWords[i],\n",
    "        'feature': features[i],\n",
    "    }\n",
    "    train_data.append(res)\n",
    "train_data = pd.DataFrame(train_data)\n",
    "input_train = np.concatenate((bagOfWords, features), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7492625368731564\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.72      0.74       502\n",
      "           1       0.74      0.78      0.76       515\n",
      "\n",
      "   micro avg       0.75      0.75      0.75      1017\n",
      "   macro avg       0.75      0.75      0.75      1017\n",
      "weighted avg       0.75      0.75      0.75      1017\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:8: FutureWarning:\n",
      "\n",
      "Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LR = LogisticRegressionCV(cv=5, max_iter=4000,random_state=0, solver='lbfgs',multi_class='multinomial', class_weight='balanced')\n",
    "clf = LR.fit(input_train, y_train)\n",
    "X_test = pd.DataFrame(X_test)\n",
    "X_test.columns = ['features', 'group', 'text', 'label']\n",
    "X_new_counts = count_vect.transform(X_test['text'].values)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "bagOfWords = X_new_tfidf.toarray()\n",
    "features = X_test['features'].as_matrix().tolist()\n",
    "input_test = np.concatenate((bagOfWords, features), axis=1)\n",
    "y_pred = clf.predict(input_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The top 10 corelated features for hostile/non_hostile groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 holy 0.9574169804327665\n",
      "0 emoji_face_with_tears_of_joy 0.8505113757510508\n",
      "0 shit 0.8448341866658511\n",
      "0 love 0.7580048191764939\n",
      "0 emoji_heavy_black_heart 0.6005186832629114\n",
      "0 lmao 0.5993324146151553\n",
      "0 len_1 0.5985066055574346\n",
      "0 it 0.5963867256938168\n",
      "0 sex 0.5951104931517495\n",
      "0 emoji_hundred_points_symbol 0.5646803676532116\n",
      "---------\n",
      "1 stfu 0.7725894224443299\n",
      "1 fucking 0.8395251562884973\n",
      "1 pussy 0.8682935444451275\n",
      "1 dick 0.9262600199799326\n",
      "1 faggot 0.9547985668923858\n",
      "1 shut 1.0868778370613563\n",
      "1 ass 1.5180356618088786\n",
      "1 retard 1.6404435751802044\n",
      "1 twat 2.1311659991288074\n",
      "1 cunt 2.8506832375993425\n"
     ]
    }
   ],
   "source": [
    "feature_names = count_vect.get_feature_names()\n",
    "feature_names = feature_names + ['group_0', 'group_1','group_2', 'group_3']+['len_'+str(185-i) for i in range(185)]\n",
    "labelid = list(LR.classes_).index(0)\n",
    "bottomn = sorted(zip(LR.coef_[labelid], feature_names))[-10:]\n",
    "topn = sorted(zip(LR.coef_[labelid], feature_names))[:10]\n",
    "for coef, feat in topn:\n",
    "    print(0, feat, coef * -1) \n",
    "print(\"---------\")\n",
    "for coef, feat in bottomn:\n",
    "    print(1, feat, coef) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) LR model using only the relationship category + length of each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:30: FutureWarning:\n",
      "\n",
      "Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = dataPrepration(data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_data, y, test_size=0.15, random_state = 123)\n",
    "X_train = pd.DataFrame(X_train)\n",
    "X_train.columns = ['features', 'group', 'text', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:2: FutureWarning:\n",
      "\n",
      "Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6420845624385447\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.56      0.61       502\n",
      "           1       0.63      0.72      0.67       515\n",
      "\n",
      "   micro avg       0.64      0.64      0.64      1017\n",
      "   macro avg       0.64      0.64      0.64      1017\n",
      "weighted avg       0.64      0.64      0.64      1017\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:5: FutureWarning:\n",
      "\n",
      "Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LR = LogisticRegression(solver='lbfgs',multi_class='multinomial', class_weight='balanced')\n",
    "clf_notext = LR.fit(X_train['features'].as_matrix().tolist(), y_train)\n",
    "X_test = pd.DataFrame(X_test)\n",
    "X_test.columns = ['features', 'group', 'text', 'label']\n",
    "y_pred = clf_notext.predict(X_test['features'].as_matrix().tolist())\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The top 10 corelated features for hostile/non_hostile groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 len_96 0.7352415399105744\n",
      "0 len_120 0.6272377467745343\n",
      "0 len_1 0.6253751797945258\n",
      "0 len_106 0.622477628644756\n",
      "0 len_131 0.6122241549647793\n",
      "0 len_135 0.576974544082422\n",
      "0 len_155 0.4906285190311689\n",
      "0 len_21 0.48470340343170226\n",
      "0 len_183 0.4456120598960049\n",
      "0 len_102 0.43692954843252413\n",
      "---------\n",
      "1 len_85 0.33818400145552846\n",
      "1 len_60 0.3445024893368753\n",
      "1 len_64 0.3965749595400604\n",
      "1 len_4 0.449481716661065\n",
      "1 len_126 0.4989853179195205\n",
      "1 len_70 0.5283991788352906\n",
      "1 len_136 0.5795901864500809\n",
      "1 len_110 0.6648328740637315\n",
      "1 len_114 0.6857273531214038\n",
      "1 len_48 0.8034084256504896\n"
     ]
    }
   ],
   "source": [
    "feature_names = ['group_0', 'group_1','group_2', 'group_3']+['len_'+str(185-i) for i in range(185)]\n",
    "labelid = list(LR.classes_).index(0)\n",
    "bottomn = sorted(zip(LR.coef_[labelid], feature_names))[-10:]\n",
    "topn = sorted(zip(LR.coef_[labelid], feature_names))[:10]\n",
    "for coef, feat in topn:\n",
    "    print(0, feat, coef * -1) \n",
    "print(\"---------\")\n",
    "for coef, feat in bottomn:\n",
    "    print(1, feat, coef) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) four individual class for the four different relationship type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1) LR model for No_friendship relationship category (00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "No_friendship_data = data.loc[(data['group'] == '0')]\n",
    "X = No_friendship_data['text'].map(lambda x: preprocess(x))\n",
    "y = No_friendship_data['hostile']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6447368421052632\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.28      0.30        60\n",
      "           1       0.75      0.77      0.76       168\n",
      "\n",
      "   micro avg       0.64      0.64      0.64       228\n",
      "   macro avg       0.53      0.53      0.53       228\n",
      "weighted avg       0.64      0.64      0.64       228\n",
      "\n"
     ]
    }
   ],
   "source": [
    "col_names = ['tweet', 'hostility']\n",
    "y=y.astype('int')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state = 123)\n",
    "nb = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('LR', LogisticRegressionCV(cv=5, random_state=0,max_iter=5000, solver='lbfgs',multi_class='multinomial', class_weight='balanced')),\n",
    "              ])\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The top 10 corelated words for hostile/non_hostile groups in the No_friendship category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 holy 2.446142254636991\n",
      "0 sex 1.6364969429372238\n",
      "0 year 1.6185930093757581\n",
      "0 af 1.5098355638413636\n",
      "0 calling 1.4959914534672643\n",
      "0 kicks 1.4697137225391033\n",
      "0 good 1.3388257344607206\n",
      "0 allowances 1.2954928568817081\n",
      "0 man 1.292666793529395\n",
      "0 poor 1.292140696543601\n",
      "---------\n",
      "1 racist 1.1289965049489172\n",
      "1 idiot 1.1292741901336472\n",
      "1 shut 1.4398712813815069\n",
      "1 dumb 1.4674769813726134\n",
      "1 retard 1.4691623174526196\n",
      "1 bitch 1.5831586508125148\n",
      "1 cunt 1.6160741298876946\n",
      "1 ass 1.6506006982341048\n",
      "1 stupid 1.6698666591667308\n",
      "1 twat 2.413823417734113\n"
     ]
    }
   ],
   "source": [
    "classifier = nb.named_steps['LR']\n",
    "vectorizer = nb.named_steps['vect']\n",
    "most_informative_feature_for_class(vectorizer, classifier, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2) LR model for dual_friendship relationship category (11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "dual_friendship_data = data.loc[(data['group'] == '11')]\n",
    "X = dual_friendship_data['text'].map(lambda x: preprocess(x))\n",
    "y = dual_friendship_data['hostile']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7155963302752294\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.78      0.79       228\n",
      "           1       0.53      0.57      0.55        99\n",
      "\n",
      "   micro avg       0.72      0.72      0.72       327\n",
      "   macro avg       0.67      0.67      0.67       327\n",
      "weighted avg       0.72      0.72      0.72       327\n",
      "\n"
     ]
    }
   ],
   "source": [
    "col_names = ['tweet', 'hostility']\n",
    "y=y.astype('int')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state = 123)\n",
    "nb = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('LR', LogisticRegressionCV(cv=5, random_state=0,max_iter=5000, solver='lbfgs',multi_class='multinomial', class_weight='balanced')),\n",
    "              ])\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The top 10 corelated words for hostile/non_hostile groups in dual_frienship category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 emoji_face_with_tears_of_joy 0.8780989620857048\n",
      "0 love 0.7539996081746743\n",
      "0 shit 0.6296379932848155\n",
      "0 lmao 0.5963574626448318\n",
      "0 need 0.5073230990232419\n",
      "0 sleep 0.4679403313887631\n",
      "0 crazy 0.4529345636144052\n",
      "0 didn 0.4475066562951526\n",
      "0 know 0.4184010209704142\n",
      "0 lol 0.40443323764212435\n",
      "---------\n",
      "1 fuck 0.528474164427784\n",
      "1 he 0.53353971614079\n",
      "1 attention 0.5506326528727972\n",
      "1 gay 0.5917764575627112\n",
      "1 big 0.5955822018051579\n",
      "1 shut 0.727168857038017\n",
      "1 ass 0.9714283576740315\n",
      "1 retard 1.107097631026854\n",
      "1 twat 1.226268692889429\n",
      "1 cunt 1.9329277874652975\n"
     ]
    }
   ],
   "source": [
    "classifier = nb.named_steps['LR']\n",
    "vectorizer = nb.named_steps['vect']\n",
    "most_informative_feature_for_class(vectorizer, classifier, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3) LR model for sender_follow_target  relationship category (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "sender_follow_target_data = data.loc[(data['group'] == '10')]\n",
    "X = sender_follow_target_data['text'].map(lambda x: preprocess(x))\n",
    "y = sender_follow_target_data['hostile']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7272727272727273\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.73      0.71       109\n",
      "           1       0.77      0.72      0.74       133\n",
      "\n",
      "   micro avg       0.73      0.73      0.73       242\n",
      "   macro avg       0.73      0.73      0.73       242\n",
      "weighted avg       0.73      0.73      0.73       242\n",
      "\n"
     ]
    }
   ],
   "source": [
    "col_names = ['tweet', 'hostility']\n",
    "y=y.astype('int')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state = 123)\n",
    "nb = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('LR', LogisticRegressionCV(cv=5, random_state=0,max_iter=5000, solver='lbfgs',multi_class='multinomial', class_weight='balanced')),\n",
    "              ])\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The top 10 corelated words for hostile/non_hostile groups for sender_follow_target category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 emoji_face_with_tears_of_joy 0.7839834259421249\n",
      "0 shit 0.734719591321404\n",
      "0 nigga 0.6588222218033988\n",
      "0 holy 0.5577799483141853\n",
      "0 emoji_loudly_crying_face 0.49526572476902836\n",
      "0 im 0.42472638353947356\n",
      "0 actual 0.41251940510392565\n",
      "0 emoji_smiling_face_with_heart__shaped_eyes 0.40932017284766276\n",
      "0 thinking 0.3987249867297563\n",
      "0 broke 0.3932755775623923\n",
      "---------\n",
      "1 real 0.44143423105868784\n",
      "1 dick 0.44983789282949477\n",
      "1 big 0.4579865842862346\n",
      "1 faggot 0.5989484693575606\n",
      "1 pussy 0.684587864569647\n",
      "1 retard 0.6951833349162585\n",
      "1 fucking 0.6981063659033089\n",
      "1 ass 0.9632938378912917\n",
      "1 twat 1.7240426807526485\n",
      "1 cunt 2.186140312864676\n"
     ]
    }
   ],
   "source": [
    "classifier = nb.named_steps['LR']\n",
    "vectorizer = nb.named_steps['vect']\n",
    "most_informative_feature_for_class(vectorizer, classifier, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4) LR model for target_follow_sender relationship category  (01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_follow_sender_data = data.loc[(data['group'] == '1')]\n",
    "X = target_follow_sender_data['text'].map(lambda x: preprocess(x))\n",
    "y = target_follow_sender_data['hostile']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6832579185520362\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.63      0.62        89\n",
      "           1       0.74      0.72      0.73       132\n",
      "\n",
      "   micro avg       0.68      0.68      0.68       221\n",
      "   macro avg       0.67      0.67      0.67       221\n",
      "weighted avg       0.69      0.68      0.68       221\n",
      "\n"
     ]
    }
   ],
   "source": [
    "col_names = ['tweet', 'hostility']\n",
    "y=y.astype('int')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state = 123)\n",
    "nb = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('LR', LogisticRegressionCV(cv=5, max_iter=5000, random_state=0, solver='lbfgs',multi_class='multinomial', class_weight='balanced')),\n",
    "              ])\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The top 10 corelated words for hostile/non_hostile groups for target_follow_sender category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 shit 1.1984192675570837\n",
      "0 emoji_face_with_tears_of_joy 0.5633359636902002\n",
      "0 love 0.509795692970128\n",
      "0 emoji_emoji_modifier_fitzpatrick_type__5 0.45713565956720925\n",
      "0 right 0.43344565799124096\n",
      "0 sex 0.41785966285665765\n",
      "0 blank_comment 0.40657350541648424\n",
      "0 rape 0.3849537961508587\n",
      "0 lol 0.37461174247070766\n",
      "0 holy 0.36988715979678305\n",
      "---------\n",
      "1 fuck 0.40459638857169067\n",
      "1 pussy 0.46191341522292784\n",
      "1 fucking 0.49116252921630105\n",
      "1 shut 0.5097111620997169\n",
      "1 faggot 0.6776075800224274\n",
      "1 bitch 0.7479201436634332\n",
      "1 ass 0.9186053986626841\n",
      "1 twat 1.0780792839001838\n",
      "1 retard 1.084588989046295\n",
      "1 cunt 1.1909235219605536\n"
     ]
    }
   ],
   "source": [
    "classifier = nb.named_steps['LR']\n",
    "vectorizer = nb.named_steps['vect']\n",
    "most_informative_feature_for_class(vectorizer, classifier, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
